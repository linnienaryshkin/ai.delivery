> Artificial Intelligence is the capacity of computers or other machines to exhibit or simulate intelligent behaviour; the field of study concerned with this. In later use also: software used to perform tasks or produce output previously thought to require human intelligence, esp. by using machine learning to extrapolate from large collections of data.

Oxford Dictionary

---

1957 **The Perceptron**

> Created by Frank Rosenblatt, the perceptron was one of the first artificial neural networks ever developed. It was able to perform binary classifications and is considered the foundation for all subsequent developments in neural networks.

1965 **Fuzzy Sets**

> Lotfi Zadeh published his seminal paper, "Fuzzy Sets," in 1965, which laid the groundwork for what would later be known as fuzzy logic. His theory argued that people do not require precise, numerical information input, and they are capable of handling a high degree of complexity despite imprecise and fuzzy information. This observation was critical, especially in fields where human decision-making is often based on vague and imprecise information.

1987 **Control Systems**

> One of the first and most famous applications of fuzzy logic was in controlling systems, such as electronic appliances and motors. In 1987, the first commercially available fuzzy logic-controlled product, a washing machine by Matsushita Electric, was introduced in Japan. This washing machine used fuzzy logic to determine the wash cycle length and water intake necessary by assessing the dirtiness of the clothes, leading to better energy and water usage.

1986 **Backpropagation**

> Although initially introduced in the 1970s, backpropagation gained prominence in the 1980s when David Rumelhart, Geoffrey Hinton, and Ronald Williams published a paper explaining how this algorithm could efficiently train multi-layer neural networks. This method is fundamental in the training of deep learning models.

1998 **LeNet-5**

> Developed by Yann LeCun, LeNet-5 was a pioneering convolutional neural network (CNN) that could recognize digits, pave the way for machine vision, and influence future designs of deeper CNNs.

2012 **AlexNet**

> AlexNet was a significant breakthrough in the field of deep learning for visual recognition tasks. Designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, this model won the ImageNet Large Scale Visual Recognition Challenge by a large margin. It highlighted the potential of deep CNNs when combined with ReLU activation and GPU computing.

2014 **Seq2Seq**

> Introduced by Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, this model architecture was pivotal for tasks that involved generating sequences from sequences, such as machine translation. This work laid the groundwork for attention mechanisms and other complex sequence modeling that would later be integral to transformer models.

2015 **Attention Mechanism**

> Introduced by Bahdanau et al., the attention mechanism allowed models to focus on different parts of the input sequence when predicting each word of the output sequence, improving the performance of neural machine translation systems. This innovation was crucial for the development of transformers.

2017 **Transformers**

> The introduction of the transformer model by Vaswani et al. marked a departure from recurrent layers used in previous models. The transformer uses self-attention mechanisms to weigh the importance of different parts of the input data without regard to their sequential order, leading to significant improvements in processing efficiency and model performance across a range of tasks.
